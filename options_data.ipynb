{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86bc8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # app/main.py\n",
    "# import os\n",
    "# import time\n",
    "# import json\n",
    "# import bisect\n",
    "# import tempfile\n",
    "# from datetime import datetime\n",
    "# from typing import List, Optional, Dict, Any\n",
    "\n",
    "# import pandas as pd\n",
    "# from fastapi import FastAPI, HTTPException, BackgroundTasks, Query\n",
    "# from pydantic import BaseModel, Field\n",
    "\n",
    "# # External provider functions from nsepython\n",
    "# # make sure you have `nsepython` installed in your env:\n",
    "# # pip install nsepython\n",
    "# from nsepython import option_chain, nse_quote\n",
    "\n",
    "# # ---------------------------\n",
    "# # Models\n",
    "# # ---------------------------\n",
    "\n",
    "# class IndexPriceResponse(BaseModel):\n",
    "#     symbol: str\n",
    "#     lastPrice: float\n",
    "#     pChange: float\n",
    "#     change: float\n",
    "#     timestamp: str\n",
    "\n",
    "# class StockPriceResponse(BaseModel):\n",
    "#     symbol: str\n",
    "#     companyName: Optional[str] = None\n",
    "#     lastPrice: Optional[float] = None\n",
    "#     pChange: Optional[float] = None\n",
    "#     change: Optional[float] = None\n",
    "#     timestamp: str\n",
    "\n",
    "# class FetchOptionsRequest(BaseModel):\n",
    "#     index: str = Field(..., description=\"Index symbol, e.g. NIFTY or BANKNIFTY\")\n",
    "#     num_strikes: int = Field(25, gt=0, le=500)\n",
    "\n",
    "# class FetchExpiryRequest(BaseModel):\n",
    "#     index: str\n",
    "#     expiry: str\n",
    "#     num_strikes: int = Field(25, gt=0, le=500)\n",
    "\n",
    "# class FetchResultMeta(BaseModel):\n",
    "#     createdAtUTC: str\n",
    "#     indexName: str\n",
    "#     nearestExpiry: Optional[str] = None\n",
    "#     selectedExpiry: Optional[str] = None\n",
    "#     underlyingValue: Optional[float] = None\n",
    "#     atmStrike: Optional[int] = None\n",
    "#     selectedStrikesRange: Optional[List[int]] = None\n",
    "#     totalStrikesFetched: Optional[int] = None\n",
    "\n",
    "# class AnalyticsResponse(BaseModel):\n",
    "#     meta: FetchResultMeta\n",
    "#     pcr: Dict[str, float]\n",
    "#     top_oi: Dict[str, List[Dict[str, Any]]]\n",
    "#     max_pain: Dict[str, Any]\n",
    "\n",
    "# # ---------------------------\n",
    "# # App & config\n",
    "# # ---------------------------\n",
    "\n",
    "# app = FastAPI(title=\"Option Chain API\", version=\"1.0\",\n",
    "#               description=\"Fetch option chains from NSE and return analytics (PCR / MaxPain / Top OI).\")\n",
    "\n",
    "# OUTPUT_DIR = os.environ.get(\"OPTION_OUTPUT_DIR\", \"option_chain_data\")\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# # ---------------------------\n",
    "# # Helpers (adapted from your script)\n",
    "# # ---------------------------\n",
    "\n",
    "# def _expand_side(df: pd.DataFrame, side: str) -> pd.DataFrame:\n",
    "#     valid_rows = df[df[side].apply(lambda x: isinstance(x, dict))]\n",
    "#     if valid_rows.empty:\n",
    "#         return pd.DataFrame()\n",
    "#     side_data = valid_rows[side].apply(pd.Series)\n",
    "#     side_data = side_data.add_prefix(f'{side}_')\n",
    "#     return side_data\n",
    "\n",
    "# def _atomic_write_csv(df: pd.DataFrame, target_path: str):\n",
    "#     # write to temp file then atomically replace\n",
    "#     dirpath = os.path.dirname(target_path)\n",
    "#     os.makedirs(dirpath, exist_ok=True)\n",
    "#     with tempfile.NamedTemporaryFile(mode=\"w\", dir=dirpath, delete=False, suffix=\".csv\") as tmp:\n",
    "#         tmp_name = tmp.name\n",
    "#         df.to_csv(tmp_name, index=False)\n",
    "#     os.replace(tmp_name, target_path)\n",
    "\n",
    "# def _atomic_write_json(obj: dict, target_path: str):\n",
    "#     dirpath = os.path.dirname(target_path)\n",
    "#     os.makedirs(dirpath, exist_ok=True)\n",
    "#     with tempfile.NamedTemporaryFile(mode=\"w\", dir=dirpath, delete=False, suffix=\".json\", encoding=\"utf-8\") as tmp:\n",
    "#         tmp_name = tmp.name\n",
    "#         json.dump(obj, tmp, indent=2)\n",
    "#     os.replace(tmp_name, target_path)\n",
    "\n",
    "# def _normalize_index_name(index: str) -> str:\n",
    "#     if not index:\n",
    "#         return \"\"\n",
    "#     s = index.strip().upper()\n",
    "#     if s in (\"NIFTY50\", \"NIFTY\", \"NSEI\"):\n",
    "#         return \"NIFTY\"\n",
    "#     if s in (\"BANKNIFTY\", \"NSEBANK\"):\n",
    "#         return \"BANKNIFTY\"\n",
    "#     return s\n",
    "\n",
    "# # ---------------------------\n",
    "# # Analytical helpers (your logic)\n",
    "# # ---------------------------\n",
    "\n",
    "# def calculate_pcr(df: pd.DataFrame) -> dict:\n",
    "#     pcr_data = {'pcr_by_oi': 0.0, 'pcr_by_volume': 0.0}\n",
    "#     if 'PE_openInterest' in df.columns and 'CE_openInterest' in df.columns:\n",
    "#         total_pe_oi = df['PE_openInterest'].fillna(0).sum()\n",
    "#         total_ce_oi = df['CE_openInterest'].fillna(0).sum()\n",
    "#         if total_ce_oi > 0:\n",
    "#             pcr_data['pcr_by_oi'] = round(total_pe_oi / total_ce_oi, 2)\n",
    "#     if 'PE_totalTradedVolume' in df.columns and 'CE_totalTradedVolume' in df.columns:\n",
    "#         total_pe_volume = df['PE_totalTradedVolume'].fillna(0).sum()\n",
    "#         total_ce_volume = df['CE_totalTradedVolume'].fillna(0).sum()\n",
    "#         if total_ce_volume > 0:\n",
    "#             pcr_data['pcr_by_volume'] = round(total_pe_volume / total_ce_volume, 2)\n",
    "#     return pcr_data\n",
    "\n",
    "# def find_high_oi_strikes(df: pd.DataFrame, top_n: int = 5) -> dict:\n",
    "#     results = {'resistance_strikes': [], 'support_strikes': []}\n",
    "#     if 'CE_openInterest' in df.columns:\n",
    "#         top_calls = df.nlargest(top_n, 'CE_openInterest')[['strikePrice', 'CE_openInterest']].fillna(0)\n",
    "#         results['resistance_strikes'] = top_calls.to_dict('records')\n",
    "#     if 'PE_openInterest' in df.columns:\n",
    "#         top_puts = df.nlargest(top_n, 'PE_openInterest')[['strikePrice', 'PE_openInterest']].fillna(0)\n",
    "#         results['support_strikes'] = top_puts.to_dict('records')\n",
    "#     return results\n",
    "\n",
    "# def calculate_max_pain(df: pd.DataFrame) -> dict:\n",
    "#     if 'strikePrice' not in df.columns:\n",
    "#         return {'max_pain_strike': None, 'max_loss_value': 0}\n",
    "#     strikes = sorted(df['strikePrice'].dropna().unique())\n",
    "#     total_loss_at_strike = {}\n",
    "#     for strike_price in strikes:\n",
    "#         loss = 0\n",
    "#         if 'CE_openInterest' in df.columns and 'CE_lastPrice' in df.columns:\n",
    "#             ce_data = df[['strikePrice', 'CE_openInterest', 'CE_lastPrice']].dropna()\n",
    "#             for _, row in ce_data.iterrows():\n",
    "#                 if row['strikePrice'] > strike_price:\n",
    "#                     loss += (row['strikePrice'] - strike_price) * row['CE_openInterest']\n",
    "#         if 'PE_openInterest' in df.columns and 'PE_lastPrice' in df.columns:\n",
    "#             pe_data = df[['strikePrice', 'PE_openInterest', 'PE_lastPrice']].dropna()\n",
    "#             for _, row in pe_data.iterrows():\n",
    "#                 if row['strikePrice'] < strike_price:\n",
    "#                     loss += (strike_price - row['strikePrice']) * row['PE_openInterest']\n",
    "#         total_loss_at_strike[strike_price] = loss\n",
    "#     if not total_loss_at_strike:\n",
    "#         return {'max_pain_strike': None, 'max_loss_value': 0}\n",
    "#     max_pain_strike = min(total_loss_at_strike, key=total_loss_at_strike.get)\n",
    "#     return {'max_pain_strike': int(max_pain_strike), 'max_loss_value': int(total_loss_at_strike[max_pain_strike])}\n",
    "\n",
    "# # ---------------------------\n",
    "# # Core fetch + save (refactored)\n",
    "# # ---------------------------\n",
    "\n",
    "# def _prepare_option_chain_df(resp: dict, expiry: str) -> pd.DataFrame:\n",
    "#     if not (isinstance(resp, dict) and 'records' in resp and 'data' in resp['records']):\n",
    "#         raise RuntimeError(\"Invalid response structure from NSE.\")\n",
    "#     df_full = pd.DataFrame(resp['records']['data'])\n",
    "#     if df_full.empty:\n",
    "#         raise RuntimeError(\"No option chain data returned by NSE.\")\n",
    "#     if 'strikePrice' not in df_full.columns:\n",
    "#         raise RuntimeError(\"Column 'strikePrice' missing from NSE response.\")\n",
    "#     df_full['strikePrice'] = pd.to_numeric(df_full['strikePrice'], errors='coerce')\n",
    "#     df = df_full[df_full['expiryDate'] == expiry].copy()\n",
    "#     if df.empty:\n",
    "#         raise RuntimeError(f\"No data for expiry {expiry}\")\n",
    "#     ce_data = _expand_side(df, 'CE')\n",
    "#     pe_data = _expand_side(df, 'PE')\n",
    "#     df_processed = pd.concat([df[['strikePrice', 'expiryDate']].reset_index(drop=True), ce_data.reset_index(drop=True), pe_data.reset_index(drop=True)], axis=1)\n",
    "#     return df_processed\n",
    "\n",
    "# def _select_strikes_and_save(df_processed: pd.DataFrame, resp: dict, index_name: str, expiry: str, num_strikes: int) -> FetchResultMeta:\n",
    "#     underlying_value = float(resp['records'].get('underlyingValue', 0))\n",
    "#     strikes = sorted(df_processed['strikePrice'].dropna().unique())\n",
    "#     if not strikes:\n",
    "#         raise RuntimeError(\"No strikes found after processing\")\n",
    "#     atm_strike_index = bisect.bisect_left(strikes, underlying_value)\n",
    "#     if atm_strike_index > 0 and abs(strikes[atm_strike_index-1] - underlying_value) < abs(strikes[atm_strike_index] - underlying_value):\n",
    "#         atm_strike_index -= 1\n",
    "#     low_index = max(0, atm_strike_index - num_strikes)\n",
    "#     high_index = min(len(strikes) - 1, atm_strike_index + num_strikes)\n",
    "#     selected_strikes = strikes[low_index:high_index+1]\n",
    "#     df_final = df_processed[df_processed['strikePrice'].isin(selected_strikes)].sort_values(['strikePrice']).reset_index(drop=True)\n",
    "#     timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "#     safe_expiry = str(expiry).replace(' ', '_').replace('/', '-')\n",
    "#     base_filename = f\"{index_name.lower()}_option_chain_{safe_expiry}_{timestamp}\"\n",
    "#     csv_path = os.path.join(OUTPUT_DIR, f\"{base_filename}.csv\")\n",
    "#     meta_path = os.path.join(OUTPUT_DIR, f\"{base_filename}.json\")\n",
    "#     # atomic write\n",
    "#     _atomic_write_csv(df_final, csv_path)\n",
    "#     metadata = {\n",
    "#         'createdAtUTC': datetime.utcnow().isoformat(),\n",
    "#         'indexName': index_name,\n",
    "#         'nearestExpiry': expiry,\n",
    "#         'underlyingValue': float(underlying_value),\n",
    "#         'atmStrike': int(strikes[atm_strike_index]) if 0 <= atm_strike_index < len(strikes) else None,\n",
    "#         'selectedStrikesRange': [int(selected_strikes[0]), int(selected_strikes[-1])],\n",
    "#         'totalStrikesFetched': int(len(df_final))\n",
    "#     }\n",
    "#     _atomic_write_json(metadata, meta_path)\n",
    "#     return FetchResultMeta(**metadata)\n",
    "\n",
    "# def fetch_and_save_option_chain(index_name: str, num_strikes_around_atm: int = 25) -> FetchResultMeta:\n",
    "#     start_time = time.time()\n",
    "#     resp = option_chain(index_name)\n",
    "#     # find nearest expiry\n",
    "#     expiries = resp['records'].get('expiryDates', [])\n",
    "#     if not expiries:\n",
    "#         raise RuntimeError(\"No expiries in NSE response.\")\n",
    "#     nearest_expiry = expiries[0]\n",
    "#     df_processed = _prepare_option_chain_df(resp, nearest_expiry)\n",
    "#     meta = _select_strikes_and_save(df_processed, resp, index_name, nearest_expiry, num_strikes_around_atm)\n",
    "#     elapsed = time.time() - start_time\n",
    "#     app.logger.info(f\"Saved option chain for {index_name} expiry {nearest_expiry} in {elapsed:.2f}s\")\n",
    "#     return meta\n",
    "\n",
    "# def fetch_specific_expiry_option_chain(index_name: str, expiry_date: str, num_strikes_around_atm: int = 25) -> FetchResultMeta:\n",
    "#     start_time = time.time()\n",
    "#     resp = option_chain(index_name)\n",
    "#     expiries = resp['records'].get('expiryDates', [])\n",
    "#     if expiry_date not in expiries:\n",
    "#         raise HTTPException(status_code=422, detail=f\"Expiry '{expiry_date}' not available. Available: {expiries}\")\n",
    "#     df_processed = _prepare_option_chain_df(resp, expiry_date)\n",
    "#     meta = _select_strikes_and_save(df_processed, resp, index_name, expiry_date, num_strikes_around_atm)\n",
    "#     elapsed = time.time() - start_time\n",
    "#     app.logger.info(f\"Saved option chain for {index_name} expiry {expiry_date} in {elapsed:.2f}s\")\n",
    "#     return meta\n",
    "\n",
    "# # ---------------------------\n",
    "# # Provider small wrappers\n",
    "# # ---------------------------\n",
    "\n",
    "# def get_available_expiries(index_name: str) -> List[str]:\n",
    "#     try:\n",
    "#         resp = option_chain(index_name)\n",
    "#         return resp['records'].get('expiryDates', [])\n",
    "#     except Exception as e:\n",
    "#         app.logger.error(\"get_available_expiries error: %s\", e)\n",
    "#         return []\n",
    "\n",
    "# def fetch_index_price(index_name: str) -> dict:\n",
    "#     try:\n",
    "#         quote = nse_quote(index_name)\n",
    "#         if not quote or 'lastPrice' not in quote:\n",
    "#             raise HTTPException(status_code=404, detail=f\"No data for index {index_name}\")\n",
    "#         last_price = float(str(quote['lastPrice']).replace(',', ''))\n",
    "#         return {\n",
    "#             'symbol': index_name,\n",
    "#             'lastPrice': last_price,\n",
    "#             'pChange': float(quote.get('pChange', 0)),\n",
    "#             'change': float(quote.get('change', 0)),\n",
    "#             'timestamp': quote.get('secDate', datetime.now().strftime(\"%d %b %Y %H:%M:%S\"))\n",
    "#         }\n",
    "#     except HTTPException:\n",
    "#         raise\n",
    "#     except Exception as e:\n",
    "#         app.logger.exception(\"fetch_index_price error\")\n",
    "#         raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# def fetch_stock_price(stock_symbol: str) -> dict:\n",
    "#     try:\n",
    "#         quote = nse_quote(stock_symbol)\n",
    "#         info = quote.get('info', {})\n",
    "#         price_info = quote.get('priceInfo', {})\n",
    "#         if not info or not price_info:\n",
    "#             raise HTTPException(status_code=404, detail=f\"No data for stock {stock_symbol}\")\n",
    "#         last_price = price_info.get('lastPrice')\n",
    "#         try:\n",
    "#             last_price = float(last_price) if last_price is not None else None\n",
    "#         except Exception:\n",
    "#             last_price = None\n",
    "#         return {\n",
    "#             'symbol': info.get('symbol'),\n",
    "#             'companyName': info.get('companyName'),\n",
    "#             'lastPrice': last_price,\n",
    "#             'pChange': float(price_info.get('pChange', 0)) if price_info.get('pChange') is not None else None,\n",
    "#             'change': float(price_info.get('change', 0)) if price_info.get('change') is not None else None,\n",
    "#             'timestamp': quote.get('metadata', {}).get('lastUpdateTime', datetime.now().strftime(\"%d-%b-%Y %H:%M:%S\"))\n",
    "#         }\n",
    "#     except HTTPException:\n",
    "#         raise\n",
    "#     except Exception as e:\n",
    "#         app.logger.exception(\"fetch_stock_price error\")\n",
    "#         raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# # ---------------------------\n",
    "# # REST endpoints\n",
    "# # ---------------------------\n",
    "\n",
    "# @app.get(\"/expiries\", response_model=List[str])\n",
    "# def api_get_expiries(index: str = Query(..., description=\"Index symbol, e.g. NIFTY\")):\n",
    "#     idx = _normalize_index_name(index)\n",
    "#     expiries = get_available_expiries(idx)\n",
    "#     if not expiries:\n",
    "#         raise HTTPException(status_code=404, detail=f\"No expiries found for {idx}\")\n",
    "#     return expiries\n",
    "\n",
    "# @app.get(\"/index-price\", response_model=IndexPriceResponse)\n",
    "# def api_index_price(index: str = Query(..., description=\"Index symbol, e.g. NIFTY\")):\n",
    "#     idx = _normalize_index_name(index)\n",
    "#     data = fetch_index_price(idx)\n",
    "#     return IndexPriceResponse(**data)\n",
    "\n",
    "# @app.get(\"/stock-price\", response_model=StockPriceResponse)\n",
    "# def api_stock_price(symbol: str = Query(..., description=\"Stock symbol (NSE), e.g. RELIANCE\")):\n",
    "#     data = fetch_stock_price(symbol.upper())\n",
    "#     return StockPriceResponse(**data)\n",
    "\n",
    "# @app.post(\"/fetch\", response_model=FetchResultMeta, status_code=201)\n",
    "# def api_fetch_options(request: FetchOptionsRequest, background_tasks: BackgroundTasks):\n",
    "#     idx = _normalize_index_name(request.index)\n",
    "#     # we will fetch in background and return immediately: to keep simple, perform sync fetch\n",
    "#     try:\n",
    "#         meta = fetch_and_save_option_chain(idx, request.num_strikes)\n",
    "#         return meta\n",
    "#     except HTTPException:\n",
    "#         raise\n",
    "#     except Exception as e:\n",
    "#         app.logger.exception(\"api_fetch_options error\")\n",
    "#         raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# @app.post(\"/fetch/expiry\", response_model=FetchResultMeta, status_code=201)\n",
    "# def api_fetch_options_expiry(req: FetchExpiryRequest):\n",
    "#     idx = _normalize_index_name(req.index)\n",
    "#     try:\n",
    "#         meta = fetch_specific_expiry_option_chain(idx, req.expiry, req.num_strikes)\n",
    "#         return meta\n",
    "#     except HTTPException:\n",
    "#         raise\n",
    "#     except Exception as e:\n",
    "#         app.logger.exception(\"api_fetch_options_expiry error\")\n",
    "#         raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# @app.get(\"/analytics\", response_model=AnalyticsResponse)\n",
    "# def api_analytics_for_latest(index: str = Query(...), limit: int = Query(500, gt=0, le=5000)):\n",
    "#     \"\"\"\n",
    "#     Read the latest saved CSV for the index and compute analytics.\n",
    "#     \"\"\"\n",
    "#     idx = _normalize_index_name(index)\n",
    "#     files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith(f\"{idx.lower()}_\") and f.endswith('.csv')]\n",
    "#     if not files:\n",
    "#         raise HTTPException(status_code=404, detail=f\"No saved option-chain CSVs found for {idx}\")\n",
    "#     latest_file = sorted(files, reverse=True)[0]\n",
    "#     csv_path = os.path.join(OUTPUT_DIR, latest_file)\n",
    "#     try:\n",
    "#         df = pd.read_csv(csv_path)\n",
    "#     except Exception as e:\n",
    "#         app.logger.exception(\"Failed to read CSV for analytics\")\n",
    "#         raise HTTPException(status_code=500, detail=\"Failed to read saved CSV\")\n",
    "#     # apply limit\n",
    "#     if limit:\n",
    "#         df = df.head(limit)\n",
    "#     pcr = calculate_pcr(df)\n",
    "#     top_oi = find_high_oi_strikes(df, top_n=5)\n",
    "#     max_pain = calculate_max_pain(df)\n",
    "#     # load metadata JSON if present\n",
    "#     meta_file = csv_path.replace('.csv', '.json')\n",
    "#     meta_obj = {}\n",
    "#     if os.path.exists(meta_file):\n",
    "#         with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "#             meta_obj = json.load(f)\n",
    "#     meta_obj.setdefault('createdAtUTC', datetime.utcnow().isoformat())\n",
    "#     meta = FetchResultMeta(**meta_obj)\n",
    "#     return AnalyticsResponse(meta=meta, pcr=pcr, top_oi=top_oi, max_pain=max_pain)\n",
    "\n",
    "# # ---------------------------\n",
    "# # Simple health endpoint\n",
    "# # ---------------------------\n",
    "\n",
    "# @app.get(\"/health\")\n",
    "# def health():\n",
    "#     return {\"status\": \"ok\", \"time\": datetime.utcnow().isoformat()}\n",
    "\n",
    "# # ---------------------------\n",
    "# # Run example if run directly\n",
    "# # ---------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     import uvicorn\n",
    "#     uvicorn.run(\"app.main:app\", host=\"127.0.0.1\", port=8000, reload=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83138bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (0.2.65)\n",
      "Requirement already satisfied: pandas in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: requests in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (4.13.5)\n",
      "Requirement already satisfied: lxml in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (5.4.0)\n",
      "Requirement already satisfied: tqdm in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: nest_asyncio in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.16.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (2.3.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (3.18.2)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (6.32.0)\n",
      "Requirement already satisfied: websockets>=13.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: colorama in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
      "Requirement already satisfied: pycparser in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: crawl4ai in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (24.1.0)\n",
      "Requirement already satisfied: aiohttp>=3.11.11 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite~=0.20 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (0.21.0)\n",
      "Requirement already satisfied: anyio>=4.0.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (4.10.0)\n",
      "Requirement already satisfied: lxml~=5.3 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (5.4.0)\n",
      "Requirement already satisfied: litellm>=1.53.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (1.77.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (2.3.3)\n",
      "Requirement already satisfied: pillow>=10.4 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (11.3.0)\n",
      "Requirement already satisfied: playwright>=1.49.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (1.55.0)\n",
      "Requirement already satisfied: patchright>=1.49.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (1.55.1)\n",
      "Requirement already satisfied: python-dotenv~=1.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (1.1.1)\n",
      "Requirement already satisfied: requests~=2.26 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4~=4.12 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (4.13.5)\n",
      "Requirement already satisfied: tf-playwright-stealth>=1.1.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (1.2.0)\n",
      "Requirement already satisfied: xxhash~=3.4 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (3.5.0)\n",
      "Requirement already satisfied: rank-bm25~=0.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (0.2.2)\n",
      "Requirement already satisfied: snowballstemmer~=2.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (2.2.0)\n",
      "Requirement already satisfied: pydantic>=2.10 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (2.11.7)\n",
      "Requirement already satisfied: pyOpenSSL>=24.3.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (25.1.0)\n",
      "Requirement already satisfied: psutil>=6.1.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (7.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (6.0.2)\n",
      "Requirement already satisfied: nltk>=3.9.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (3.9.1)\n",
      "Requirement already satisfied: rich>=13.9.4 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (14.1.0)\n",
      "Requirement already satisfied: httpx>=0.27.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (0.28.1)\n",
      "Requirement already satisfied: fake-useragent>=2.0.3 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.7 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (8.2.1)\n",
      "Requirement already satisfied: chardet>=5.2.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (5.2.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (1.1.0)\n",
      "Requirement already satisfied: humanize>=4.10.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (4.13.0)\n",
      "Requirement already satisfied: lark>=1.2.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (1.2.2)\n",
      "Requirement already satisfied: alphashape>=1.3.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: shapely>=2.0.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from crawl4ai) (2.1.1)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from aiosqlite~=0.20->crawl4ai) (4.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from beautifulsoup4~=4.12->crawl4ai) (2.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from requests~=2.26->crawl4ai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from requests~=2.26->crawl4ai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from requests~=2.26->crawl4ai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from requests~=2.26->crawl4ai) (2025.8.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (1.20.1)\n",
      "Requirement already satisfied: click-log>=0.3.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from alphashape>=1.3.1->crawl4ai) (0.4.0)\n",
      "Requirement already satisfied: trimesh>=3.9.8 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from alphashape>=1.3.1->crawl4ai) (4.8.1)\n",
      "Requirement already satisfied: networkx>=2.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from alphashape>=1.3.1->crawl4ai) (3.5)\n",
      "Requirement already satisfied: rtree>=0.9.7 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from alphashape>=1.3.1->crawl4ai) (1.4.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from alphashape>=1.3.1->crawl4ai) (1.16.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from anyio>=4.0.0->crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: colorama in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from click>=8.1.7->crawl4ai) (0.4.6)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from httpx>=0.27.2->crawl4ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.2->crawl4ai) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from httpx[http2]>=0.27.2->crawl4ai) (4.3.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.27.2->crawl4ai) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.27.2->crawl4ai) (4.1.0)\n",
      "Requirement already satisfied: fastuuid>=0.12.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (8.7.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (4.25.1)\n",
      "Requirement already satisfied: openai>=1.99.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (1.107.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (0.11.0)\n",
      "Requirement already satisfied: tokenizers in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (0.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from pydantic>=2.10->crawl4ai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from pydantic>=2.10->crawl4ai) (0.4.1)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.23.0)\n",
      "Requirement already satisfied: joblib in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from nltk>=3.9.1->crawl4ai) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from nltk>=3.9.1->crawl4ai) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from nltk>=3.9.1->crawl4ai) (4.67.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from openai>=1.99.5->litellm>=1.53.1->crawl4ai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from openai>=1.99.5->litellm>=1.53.1->crawl4ai) (0.10.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from patchright>=1.49.0->crawl4ai) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from patchright>=1.49.0->crawl4ai) (3.2.4)\n",
      "Requirement already satisfied: cryptography<46,>=41.0.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from pyOpenSSL>=24.3.0->crawl4ai) (45.0.7)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from cryptography<46,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (2.0.0)\n",
      "Requirement already satisfied: pycparser in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from cffi>=1.14->cryptography<46,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (2.23)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from rich>=13.9.4->crawl4ai) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from rich>=13.9.4->crawl4ai) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->crawl4ai) (0.1.2)\n",
      "Requirement already satisfied: fake-http-header<0.4.0,>=0.3.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from tf-playwright-stealth>=1.1.0->crawl4ai) (0.3.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.34.4)\n",
      "Requirement already satisfied: filelock in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2025.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance pandas requests beautifulsoup4 lxml tqdm nest_asyncio\n",
    "# Optional (only if you want Crawl4AI fallback and have access)\n",
    "!pip install crawl4ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be09a824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting final SENSEX option chain fetch. Output dir: D:\\WORKSPACE\\COMPITITION\\STOCK_APIS\\FastAPI-Stock-data-\\sensex_option_chain_out\n",
      "[attempt] yfinance: ^BSESN\n",
      "[yfinance] failed: yfinance returned no expiries for symbol\n",
      "[attempt] yahoo json endpoint: ^BSESN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\omcho\\AppData\\Local\\Temp\\ipykernel_19692\\1416549496.py\", line 277, in orchestrator\n",
      "    out = attempt_yfinance(symbol)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\omcho\\AppData\\Local\\Temp\\ipykernel_19692\\1416549496.py\", line 66, in attempt_yfinance\n",
      "    raise RuntimeError(\"yfinance returned no expiries for symbol\")\n",
      "RuntimeError: yfinance returned no expiries for symbol\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  429 rate limited by Yahoo (attempt 1/4), backing off 2s\n",
      "  429 rate limited by Yahoo (attempt 2/4), backing off 4s\n",
      "  429 rate limited by Yahoo (attempt 3/4), backing off 8s\n",
      "  429 rate limited by Yahoo (attempt 4/4), backing off 16s\n",
      "[yahoo_json] failed: Yahoo JSON failed after retries (429 or network)\n",
      "[attempt] parse saved HTML: sensex_option_chain_out\\crawl4ai_all_rendered_20250910_184613.html\n",
      "No extracted tables/json from saved html: sensex_option_chain_out\\crawl4ai_all_rendered_20250910_184613.html\n",
      "[attempt] Playwright rendering (sync) for pages: ['https://www.bseindia.com/markets/Derivatives/DeriReports/DeriOptionchain.html', 'https://web.sensibull.com/option-chain?tradingsymbol=SENSEX', 'https://upstox.com/option-chain/sensex/', 'https://www.tradingview.com/symbols/BSE-SENSEX/options-chain/']\n",
      "[playwright] failed: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "Final failure: All automated methods failed. Inspect files in ./sensex_option_chain_out/ for debug HTML/JSON.\n",
      "Check folder ./sensex_option_chain_out/ for debug files (html/json).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\omcho\\AppData\\Local\\Temp\\ipykernel_19692\\1416549496.py\", line 291, in orchestrator\n",
      "    out = attempt_yahoo_json(symbol)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\omcho\\AppData\\Local\\Temp\\ipykernel_19692\\1416549496.py\", line 103, in attempt_yahoo_json\n",
      "    raise RuntimeError(\"Yahoo JSON failed after retries (429 or network)\")\n",
      "RuntimeError: Yahoo JSON failed after retries (429 or network)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\omcho\\AppData\\Local\\Temp\\ipykernel_19692\\1416549496.py\", line 338, in orchestrator\n",
      "    dfs = render_with_playwright_and_extract(pages)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\omcho\\AppData\\Local\\Temp\\ipykernel_19692\\1416549496.py\", line 241, in render_with_playwright_and_extract\n",
      "    with sync_playwright() as p:\n",
      "  File \"d:\\WORKSPACE\\COMPITITION\\STOCK_APIS\\.venv\\Lib\\site-packages\\playwright\\sync_api\\_context_manager.py\", line 47, in __enter__\n",
      "    raise Error(\n",
      "playwright._impl._errors.Error: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\omcho\\AppData\\Local\\Temp\\ipykernel_19692\\1416549496.py\", line 359, in <module>\n",
      "    result = orchestrator()\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\omcho\\AppData\\Local\\Temp\\ipykernel_19692\\1416549496.py\", line 350, in orchestrator\n",
      "    raise RuntimeError(\"All automated methods failed. Inspect files in ./sensex_option_chain_out/ for debug HTML/JSON.\")\n",
      "RuntimeError: All automated methods failed. Inspect files in ./sensex_option_chain_out/ for debug HTML/JSON.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Final robust fetcher for SENSEX option chain (nearest expiry).\n",
    "Saves CSV + JSON outputs to ./sensex_option_chain_out/\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Optional libs\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    YFINANCE_AVAILABLE = True\n",
    "except Exception:\n",
    "    yf = None\n",
    "    YFINANCE_AVAILABLE = False\n",
    "\n",
    "# Optional Playwright (sync)\n",
    "try:\n",
    "    from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout\n",
    "    PLAYWRIGHT_AVAILABLE = True\n",
    "except Exception:\n",
    "    sync_playwright = None\n",
    "    PlaywrightTimeout = Exception\n",
    "    PLAYWRIGHT_AVAILABLE = False\n",
    "\n",
    "OUTDIR = Path(\"./sensex_option_chain_out\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def save_json(obj, fname):\n",
    "    p = OUTDIR / fname\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "    print(\"[saved json]\", p)\n",
    "    return p\n",
    "\n",
    "\n",
    "def save_csv(df, fname):\n",
    "    p = OUTDIR / fname\n",
    "    df.to_csv(p, index=False)\n",
    "    print(\"[saved csv ]\", p)\n",
    "    return p\n",
    "\n",
    "\n",
    "# ---------------- Method A: yfinance ----------------\n",
    "def attempt_yfinance(symbol=\"^BSESN\"):\n",
    "    if not YFINANCE_AVAILABLE:\n",
    "        raise RuntimeError(\"yfinance not installed\")\n",
    "    print(\"[attempt] yfinance:\", symbol)\n",
    "    t = yf.Ticker(symbol)\n",
    "    exps = getattr(t, \"options\", None)\n",
    "    if not exps:\n",
    "        raise RuntimeError(\"yfinance returned no expiries for symbol\")\n",
    "    nearest = exps[0]\n",
    "    print(\"  nearest expiry (yfinance):\", nearest)\n",
    "    oc = t.option_chain(nearest)\n",
    "    calls = oc.calls.copy(); puts = oc.puts.copy()\n",
    "    calls[\"side\"] = \"CALL\"; puts[\"side\"] = \"PUT\"\n",
    "    df = pd.concat([calls, puts], ignore_index=True, sort=False)\n",
    "    meta = {\"source\": \"yfinance\", \"symbol\": symbol, \"nearest_expiry\": nearest, \"retrieved_at\": datetime.utcnow().isoformat() + \"Z\"}\n",
    "    raw = {\"calls\": calls.to_dict(orient=\"records\"), \"puts\": puts.to_dict(orient=\"records\")}\n",
    "    return {\"meta\": meta, \"df\": df, \"raw\": raw}\n",
    "\n",
    "\n",
    "# ---------------- Method B: Yahoo JSON with retries ----------------\n",
    "def attempt_yahoo_json(symbol=\"^BSESN\", max_retries=4):\n",
    "    print(\"[attempt] yahoo json endpoint:\", symbol)\n",
    "    base = f\"https://query2.finance.yahoo.com/v7/finance/options/{symbol}\"\n",
    "    tries = 0\n",
    "    while tries < max_retries:\n",
    "        tries += 1\n",
    "        try:\n",
    "            r = requests.get(base, timeout=15)\n",
    "        except Exception as e:\n",
    "            wait = 2 ** tries\n",
    "            print(f\"  request error (attempt {tries}/{max_retries}), retry in {wait}s:\", e)\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            break\n",
    "        if r.status_code == 429:\n",
    "            wait = 2 ** tries\n",
    "            print(f\"  429 rate limited by Yahoo (attempt {tries}/{max_retries}), backing off {wait}s\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "        else:\n",
    "            r.raise_for_status()\n",
    "    else:\n",
    "        raise RuntimeError(\"Yahoo JSON failed after retries (429 or network)\")\n",
    "\n",
    "    j = r.json()\n",
    "    result = j.get(\"optionChain\", {}).get(\"result\")\n",
    "    if not result:\n",
    "        raise RuntimeError(\"Yahoo returned no optionChain result\")\n",
    "    obj = result[0]\n",
    "    exps = obj.get(\"expirationDates\", [])\n",
    "    if not exps:\n",
    "        raise RuntimeError(\"Yahoo returned no expiries\")\n",
    "    now_ts = int(time.time())\n",
    "    nearest_ts = next((ts for ts in sorted(exps) if ts >= now_ts), exps[-1])\n",
    "    print(\"  nearest expiry (unix):\", nearest_ts)\n",
    "\n",
    "    # fetch explicit expiry\n",
    "    tries = 0\n",
    "    while tries < max_retries:\n",
    "        tries += 1\n",
    "        r2 = requests.get(base + f\"?date={nearest_ts}\", timeout=15)\n",
    "        if r2.status_code == 200:\n",
    "            break\n",
    "        if r2.status_code == 429:\n",
    "            wait = 2 ** tries\n",
    "            print(f\"  expiry fetch 429 (attempt {tries}/{max_retries}), wait {wait}s\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "        else:\n",
    "            r2.raise_for_status()\n",
    "    else:\n",
    "        raise RuntimeError(\"Yahoo expiry fetch failed after retries\")\n",
    "\n",
    "    j2 = r2.json()\n",
    "    rr = j2.get(\"optionChain\", {}).get(\"result\", [{}])[0]\n",
    "    options_list = rr.get(\"options\", [])\n",
    "    if not options_list:\n",
    "        raise RuntimeError(\"Yahoo returned empty options list\")\n",
    "    options = options_list[0]\n",
    "    calls = options.get(\"calls\", []); puts = options.get(\"puts\", [])\n",
    "    calls_df = pd.DataFrame(calls); calls_df[\"side\"] = \"CALL\"\n",
    "    puts_df = pd.DataFrame(puts); puts_df[\"side\"] = \"PUT\"\n",
    "    df = pd.concat([calls_df, puts_df], ignore_index=True, sort=False)\n",
    "    meta = {\"source\": \"yahoo_json\", \"symbol\": symbol, \"nearest_expiry_unix\": nearest_ts, \"retrieved_at\": datetime.utcnow().isoformat()+\"Z\"}\n",
    "    raw = {\"calls\": calls, \"puts\": puts, \"full\": rr}\n",
    "    return {\"meta\": meta, \"df\": df, \"raw\": raw}\n",
    "\n",
    "\n",
    "# ---------------- Method C: Parse saved rendered HTML ----------------\n",
    "def parse_saved_html(path: Path):\n",
    "    \"\"\"\n",
    "    Attempts to extract option chain info from a saved rendered HTML file.\n",
    "    - looks for embedded JSON-like blobs (containing option/strike/openInterest/etc)\n",
    "    - extracts largest tables into CSV\n",
    "    Returns list of extracted DataFrames (could be empty)\n",
    "    \"\"\"\n",
    "    print(\"[attempt] parse saved HTML:\", path)\n",
    "    if not path.exists():\n",
    "        raise RuntimeError(f\"Saved HTML file not found: {path}\")\n",
    "\n",
    "    html = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    extracted_dfs = []\n",
    "    # 1) extract embedded JSON blobs from scripts\n",
    "    for s in soup.find_all(\"script\"):\n",
    "        txt = s.string or \"\"\n",
    "        if not txt:\n",
    "            txt = \"\".join(s.strings)\n",
    "        low = txt.lower()\n",
    "        if any(k in low for k in (\"option\", \"strike\", \"expiration\", \"openinterest\", \"optionchain\")):\n",
    "            for m in re.finditer(r\"(\\{(?:.|\\n){120,200000}\\})\", txt):\n",
    "                blob = m.group(1)\n",
    "                # try strict JSON then tolerant fixes\n",
    "                try:\n",
    "                    parsed = json.loads(blob)\n",
    "                except Exception:\n",
    "                    cand = blob.replace(\"'\", '\"')\n",
    "                    cand = re.sub(r\",\\s*}\", \"}\", cand)\n",
    "                    cand = re.sub(r\",\\s*]\", \"]\", cand)\n",
    "                    try:\n",
    "                        parsed = json.loads(cand)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                # if parsed a dict with lists of dicts, convert the lists to DataFrame(s)\n",
    "                if isinstance(parsed, dict):\n",
    "                    # find lists of dicts (calls/puts/etc)\n",
    "                    for k, v in parsed.items():\n",
    "                        if isinstance(v, list) and v and isinstance(v[0], dict):\n",
    "                            try:\n",
    "                                df = pd.json_normalize(v)\n",
    "                                df.attrs[\"source_key\"] = k\n",
    "                                extracted_dfs.append(df)\n",
    "                                # save JSON and CSV for debugging\n",
    "                                fname_base = f\"embedded_{k}_{timestamp()}\"\n",
    "                                save_json(v, fname_base + \".json\")\n",
    "                                save_csv(df, fname_base + \".csv\")\n",
    "                            except Exception:\n",
    "                                pass\n",
    "    # 2) extract largest HTML tables\n",
    "    tables = soup.find_all(\"table\")\n",
    "    if tables:\n",
    "        def cell_count(t): return sum(len(r.find_all([\"td\", \"th\"])) for r in t.find_all(\"tr\"))\n",
    "        tables_sorted = sorted(tables, key=cell_count, reverse=True)\n",
    "        for idx, t in enumerate(tables_sorted[:3], 1):\n",
    "            rows = t.find_all(\"tr\")\n",
    "            header = None\n",
    "            data = []\n",
    "            for i, r in enumerate(rows):\n",
    "                cells = r.find_all([\"th\", \"td\"])\n",
    "                texts = [c.get_text(separator=\" \", strip=True) for c in cells]\n",
    "                if i == 0 and any(texts):\n",
    "                    header = texts\n",
    "                else:\n",
    "                    if texts and any(tt.strip() for tt in texts):\n",
    "                        data.append(texts)\n",
    "            if data:\n",
    "                maxc = max(len(r) for r in data)\n",
    "                if not header or len(header) < maxc:\n",
    "                    header = header or [f\"col{i}\" for i in range(1, maxc+1)]\n",
    "                    if len(header) < maxc:\n",
    "                        header = header + [f\"col{i}\" for i in range(len(header)+1, maxc+1)]\n",
    "                rows_norm = [r + [\"\"]*(maxc - len(r)) for r in data]\n",
    "                df = pd.DataFrame(rows_norm, columns=header)\n",
    "                fname = f\"extracted_table_{idx}_{timestamp()}\"\n",
    "                save_csv(df, fname + \".csv\")\n",
    "                extracted_dfs.append(df)\n",
    "    return extracted_dfs\n",
    "\n",
    "\n",
    "# ---------------- Optional Method D: Render page with Playwright sync ----------------\n",
    "def render_with_playwright_and_extract(urls):\n",
    "    \"\"\"\n",
    "    Uses Playwright sync API to render pages and then parse rendered HTML using same heuristics.\n",
    "    Requires Playwright + browser binaries installed.\n",
    "    \"\"\"\n",
    "    if not PLAYWRIGHT_AVAILABLE:\n",
    "        raise RuntimeError(\"playwright not installed\")\n",
    "    print(\"[attempt] Playwright rendering (sync) for pages:\", urls)\n",
    "    all_dfs = []\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        for url in urls:\n",
    "            try:\n",
    "                page = browser.new_page()\n",
    "                page.goto(url, wait_until=\"networkidle\", timeout=30000)\n",
    "                html = page.content()\n",
    "                # write HTML for debugging\n",
    "                html_file = OUTDIR / f\"playwright_page_{timestamp()}.html\"\n",
    "                html_file.write_text(html, encoding=\"utf-8\")\n",
    "                print(\"  saved rendered html:\", html_file)\n",
    "                # parse like saved HTML\n",
    "                # create soup and reuse parse logic (slightly adapted)\n",
    "                soup = BeautifulSoup(html, \"lxml\")\n",
    "                # reuse logic: extract tables and embedded json\n",
    "                # reuse parse_saved_html by writing html to temp and calling it\n",
    "                tmp_file = OUTDIR / f\"_tmp_render_{timestamp()}.html\"\n",
    "                tmp_file.write_text(html, encoding=\"utf-8\")\n",
    "                dfs = parse_saved_html(tmp_file)\n",
    "                all_dfs.extend(dfs)\n",
    "            except PlaywrightTimeout as e:\n",
    "                print(\"  Playwright timeout for\", url, e)\n",
    "            except Exception as ex:\n",
    "                print(\"  Playwright error for\", url, ex)\n",
    "        browser.close()\n",
    "    return all_dfs\n",
    "\n",
    "\n",
    "# ---------------- Orchestrator ----------------\n",
    "def orchestrator(saved_html_path: Path = None):\n",
    "    symbol = \"^BSESN\"\n",
    "    ts = timestamp()\n",
    "\n",
    "    # 1) Try yfinance\n",
    "    if YFINANCE_AVAILABLE:\n",
    "        try:\n",
    "            out = attempt_yfinance(symbol)\n",
    "            df = out[\"df\"]; meta = out[\"meta\"]; raw = out[\"raw\"]\n",
    "            save_csv(df, f\"yfinance_optionchain_{ts}.csv\")\n",
    "            save_json(raw, f\"yfinance_raw_{ts}.json\")\n",
    "            save_json(meta, f\"yfinance_meta_{ts}.json\")\n",
    "            return {\"method\": \"yfinance\", \"meta\": meta, \"df\": df, \"raw\": raw}\n",
    "        except Exception as e:\n",
    "            print(\"[yfinance] failed:\", e)\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"yfinance not available  skipping\")\n",
    "\n",
    "    # 2) Try Yahoo JSON\n",
    "    try:\n",
    "        out = attempt_yahoo_json(symbol)\n",
    "        df = out[\"df\"]; meta = out[\"meta\"]; raw = out[\"raw\"]\n",
    "        save_csv(df, f\"yahoo_optionchain_{ts}.csv\")\n",
    "        save_json(raw, f\"yahoo_raw_{ts}.json\")\n",
    "        save_json(meta, f\"yahoo_meta_{ts}.json\")\n",
    "        return {\"method\": \"yahoo_json\", \"meta\": meta, \"df\": df, \"raw\": raw}\n",
    "    except Exception as e:\n",
    "        print(\"[yahoo_json] failed:\", e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # 3) Try parse saved rendered HTML if given or auto-find latest in outdir\n",
    "    try:\n",
    "        saved_html = None\n",
    "        if saved_html_path:\n",
    "            saved_html = Path(saved_html_path)\n",
    "        else:\n",
    "            # find latest crawl4ai / playwright html in OUTDIR\n",
    "            html_files = sorted(OUTDIR.glob(\"**/*render*.html\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "            html_files += sorted(OUTDIR.glob(\"**/crawl4ai_*.html\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "            html_files += sorted(OUTDIR.glob(\"**/playwright_page_*.html\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "            if html_files:\n",
    "                saved_html = html_files[0]\n",
    "        if saved_html and saved_html.exists():\n",
    "            dfs = parse_saved_html(saved_html)\n",
    "            if dfs:\n",
    "                # pick best candidate (largest DF)\n",
    "                best = max(dfs, key=lambda d: d.shape[0]*d.shape[1])\n",
    "                save_csv(best, f\"parsed_saved_html_optionchain_{ts}.csv\")\n",
    "                save_json({\"source\": \"saved_html\", \"file\": str(saved_html), \"retrieved_at\": datetime.utcnow().isoformat()+\"Z\"}, f\"parsed_saved_html_meta_{ts}.json\")\n",
    "                return {\"method\": \"parsed_saved_html\", \"meta\": {\"file\": str(saved_html)}, \"df\": best, \"raw\": {\"html\": str(saved_html)}}\n",
    "            else:\n",
    "                print(\"No extracted tables/json from saved html:\", saved_html)\n",
    "        else:\n",
    "            print(\"No saved rendered HTML found to parse.\")\n",
    "    except Exception as e:\n",
    "        print(\"[parse_saved_html] failed:\", e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # 4) Optional: try Playwright rendering (if available)\n",
    "    if PLAYWRIGHT_AVAILABLE:\n",
    "        try:\n",
    "            pages = [\n",
    "                \"https://www.bseindia.com/markets/Derivatives/DeriReports/DeriOptionchain.html\",\n",
    "                \"https://web.sensibull.com/option-chain?tradingsymbol=SENSEX\",\n",
    "                \"https://upstox.com/option-chain/sensex/\",\n",
    "                \"https://www.tradingview.com/symbols/BSE-SENSEX/options-chain/\",\n",
    "            ]\n",
    "            dfs = render_with_playwright_and_extract(pages)\n",
    "            if dfs:\n",
    "                best = max(dfs, key=lambda d: d.shape[0]*d.shape[1])\n",
    "                save_csv(best, f\"playwright_extracted_optionchain_{ts}.csv\")\n",
    "                save_json({\"source\": \"playwright_render\", \"retrieved_at\": datetime.utcnow().isoformat()+\"Z\"}, f\"playwright_meta_{ts}.json\")\n",
    "                return {\"method\": \"playwright_render\", \"meta\": {}, \"df\": best, \"raw\": {}}\n",
    "        except Exception as e:\n",
    "            print(\"[playwright] failed:\", e)\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"Playwright not available  skipping rendering fallback\")\n",
    "\n",
    "    raise RuntimeError(\"All automated methods failed. Inspect files in ./sensex_option_chain_out/ for debug HTML/JSON.\")\n",
    "\n",
    "\n",
    "# ---------------- Run if script ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting final SENSEX option chain fetch. Output dir:\", OUTDIR.resolve())\n",
    "    try:\n",
    "        # If you already have a saved combined HTML, pass its path like:\n",
    "        # result = orchestrator(saved_html_path=Path(\"sensex_option_chain_out/crawl4ai_all_rendered_20250910_184613.html\"))\n",
    "        result = orchestrator()\n",
    "        print(\"Success. Method used:\", result.get(\"method\"))\n",
    "        print(\"Meta:\", result.get(\"meta\"))\n",
    "        df = result.get(\"df\")\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            print(\"Preview (top 20 rows):\")\n",
    "            print(df.head(20).to_string())\n",
    "        else:\n",
    "            print(\"No DataFrame returned by selected method.\")\n",
    "    except Exception as exc:\n",
    "        print(\"Final failure:\", exc)\n",
    "        traceback.print_exc()\n",
    "        print(\"Check folder ./sensex_option_chain_out/ for debug files (html/json).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8799bd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (0.2.65)\n",
      "Requirement already satisfied: pandas in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: requests in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (4.13.5)\n",
      "Requirement already satisfied: lxml in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (5.4.0)\n",
      "Requirement already satisfied: playwright in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (1.55.0)\n",
      "Requirement already satisfied: numpy>=1.16.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (2.3.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (3.18.2)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (6.32.0)\n",
      "Requirement already satisfied: websockets>=13.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from playwright) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from playwright) (3.2.4)\n",
      "Requirement already satisfied: cffi>=1.12.0 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
      "Requirement already satisfied: pycparser in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in d:\\workspace\\compitition\\stock_apis\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance pandas requests beautifulsoup4 lxml playwright\n",
    "# If you want Playwright rendering to work locally, install browser binaries:\n",
    "!python -m playwright install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9f8e9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing BSE index data...\n",
      "SENSEX info available: True\n",
      "SENSEX current price: 81425.15\n",
      "SENSEX history (last 5 days):\n",
      "                                   Open          High           Low  \\\n",
      "Date                                                                  \n",
      "2025-09-04 00:00:00+05:30  81456.671875  81456.671875  80608.937500   \n",
      "2025-09-05 00:00:00+05:30  81012.421875  81036.562500  80321.187500   \n",
      "2025-09-08 00:00:00+05:30  80904.398438  81171.382812  80733.070312   \n",
      "2025-09-09 00:00:00+05:30  81129.687500  81181.367188  80927.968750   \n",
      "2025-09-10 00:00:00+05:30  81504.359375  81643.882812  81235.421875   \n",
      "\n",
      "                                  Close  Volume  Dividends  Stock Splits  \n",
      "Date                                                                      \n",
      "2025-09-04 00:00:00+05:30  80718.007812   17000        0.0           0.0  \n",
      "2025-09-05 00:00:00+05:30  80710.757812    8200        0.0           0.0  \n",
      "2025-09-08 00:00:00+05:30  80787.296875    8700        0.0           0.0  \n",
      "2025-09-09 00:00:00+05:30  81101.320312    6300        0.0           0.0  \n",
      "2025-09-10 00:00:00+05:30  81425.148438       0        0.0           0.0  \n",
      "\n",
      "BSE Bank Index info available: True\n",
      "BSE Bank Index current price: 60998.97\n",
      "BSE Bank Index history (last 5 days):\n",
      "                                   Open          High           Low  \\\n",
      "Date                                                                  \n",
      "2025-09-10 00:00:00+05:30  61036.289062  61207.398438  60806.769531   \n",
      "\n",
      "                                 Close  Volume  Dividends  Stock Splits  \n",
      "Date                                                                     \n",
      "2025-09-10 00:00:00+05:30  60998.96875       0        0.0           0.0  \n",
      "\n",
      "^BSE100 available: None\n",
      "^BSE200 available: None\n",
      "^BSE500 available: None\n",
      "\n",
      "BSE index data is working successfully!\n",
      "SENSEX history saved to sensex_history.csv\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Test BSE index data\n",
    "print('Testing BSE index data...')\n",
    "\n",
    "# Test SENSEX (^BSESN)\n",
    "try:\n",
    "    ticker = yf.Ticker('^BSESN')\n",
    "    print('SENSEX info available:', bool(ticker.info))\n",
    "    print('SENSEX current price:', ticker.info.get('regularMarketPrice'))\n",
    "    print('SENSEX history (last 5 days):')\n",
    "    hist = ticker.history(period='5d')\n",
    "    print(hist)\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print('Error with ^BSESN:', e)\n",
    "\n",
    "# Test BSE Bank Index (BSE-BANK.BO)\n",
    "try:\n",
    "    ticker = yf.Ticker('BSE-BANK.BO')\n",
    "    print('BSE Bank Index info available:', bool(ticker.info))\n",
    "    print('BSE Bank Index current price:', ticker.info.get('regularMarketPrice'))\n",
    "    print('BSE Bank Index history (last 5 days):')\n",
    "    hist = ticker.history(period='5d')\n",
    "    print(hist)\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print('Error with BSE-BANK.BO:', e)\n",
    "\n",
    "# Test other BSE indices\n",
    "bse_indices = ['^BSE100', '^BSE200', '^BSE500']\n",
    "for symbol in bse_indices:\n",
    "    try:\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        if ticker.info:\n",
    "            print(f'{symbol} available: {ticker.info.get(\"regularMarketPrice\")}')\n",
    "        else:\n",
    "            print(f'{symbol} not available')\n",
    "    except Exception as e:\n",
    "        print(f'Error with {symbol}: {e}')\n",
    "\n",
    "print('\\nBSE index data is working successfully!')\n",
    "hist.to_csv('sensex_history.csv')\n",
    "print('SENSEX history saved to sensex_history.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318274f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
